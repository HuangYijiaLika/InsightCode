package com.example.myapplication

import android.Manifest
import android.Manifest.permission.RECORD_AUDIO
import android.content.pm.PackageManager
import android.media.AudioFormat
import android.media.AudioManager
import android.media.AudioRecord
import android.media.AudioTrack
import android.media.MediaRecorder
import android.os.Bundle
import android.speech.RecognitionListener
import android.speech.RecognizerIntent
import android.speech.SpeechRecognizer
import android.util.Base64
import android.util.Log
import androidx.activity.ComponentActivity
import androidx.activity.compose.rememberLauncherForActivityResult
import androidx.activity.compose.setContent
import androidx.activity.enableEdgeToEdge
import androidx.activity.result.contract.ActivityResultContracts
import androidx.camera.core.CameraSelector
import androidx.camera.core.Preview
import androidx.camera.lifecycle.ProcessCameraProvider
import androidx.camera.view.PreviewView
import androidx.compose.foundation.layout.Arrangement
import androidx.compose.foundation.layout.Box
import androidx.compose.foundation.layout.Column
import androidx.compose.foundation.layout.Row
import androidx.compose.foundation.layout.fillMaxSize
import androidx.compose.foundation.layout.fillMaxWidth
import androidx.compose.foundation.layout.padding
import androidx.compose.material3.Scaffold
import androidx.compose.material3.Text
import androidx.compose.runtime.Composable
import androidx.compose.runtime.DisposableEffect
import androidx.compose.runtime.getValue
import androidx.compose.runtime.mutableStateOf
import androidx.compose.runtime.remember
import androidx.compose.runtime.setValue
import androidx.compose.ui.Modifier
import android.os.Handler
import android.os.Looper
import androidx.core.content.ContextCompat
import androidx.lifecycle.LifecycleOwner
import com.example.myapplication.ui.theme.MyApplicationTheme
import com.example.myapplication.viewModule.PostViewModel
import androidx.compose.material3.Button
import androidx.compose.material3.MaterialTheme
import androidx.compose.material3.Surface
import androidx.compose.ui.unit.dp
import androidx.compose.ui.viewinterop.AndroidView
import androidx.compose.runtime.LaunchedEffect
import androidx.compose.ui.platform.LocalContext
import androidx.lifecycle.compose.LocalLifecycleOwner
import com.example.myapplication.viewModule.Content
import com.example.myapplication.viewModule.ImageUrl
import androidx.camera.core.ImageCapture
import androidx.camera.core.ImageCaptureException
import java.io.File
import java.io.FileOutputStream
import java.io.IOException
import java.util.concurrent.ExecutorService
import java.util.concurrent.Executors
import com.example.myapplication.viewModule.Message
import java.io.ByteArrayOutputStream
import android.graphics.Bitmap
import android.graphics.BitmapFactory
import android.media.Image
import java.nio.ByteBuffer
import com.example.myapplication.R
import android.content.Intent
import java.util.Locale
import androidx.compose.foundation.layout.height
import androidx.compose.ui.Alignment
import androidx.compose.foundation.layout.width
import androidx.compose.foundation.layout.wrapContentHeight
import androidx.compose.foundation.layout.wrapContentWidth
import androidx.compose.material3.Card
import androidx.compose.material3.CardDefaults
import androidx.compose.ui.text.style.TextAlign
import androidx.compose.foundation.clickable
import androidx.compose.foundation.interaction.MutableInteractionSource
import androidx.compose.foundation.interaction.PressInteraction
import androidx.compose.runtime.rememberCoroutineScope
import kotlinx.coroutines.delay
import kotlinx.coroutines.launch
import androidx.compose.foundation.gestures.detectTapGestures
import androidx.compose.ui.input.pointer.pointerInput
import androidx.compose.ui.geometry.Offset
import androidx.compose.ui.graphics.Color
import androidx.compose.foundation.shape.CircleShape
import androidx.compose.ui.unit.IntOffset
import android.speech.tts.TextToSpeech
import android.os.Vibrator
import android.content.Context
import android.os.Build
import com.google.gson.Gson
import com.example.myapplication.viewModule.AIResponseJson

class MainActivity : ComponentActivity() {
    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        enableEdgeToEdge()
        setContent {
            MyApplicationTheme {
                Surface(color = MaterialTheme.colorScheme.background) {
                    Scaffold(modifier = Modifier.fillMaxSize()) { paddingValues ->
                        PostScreen(
                            modifier = Modifier.padding(paddingValues),
                            viewModel = PostViewModel()
                        )
                    }
                }
            }
        }
    }
}

@Composable
fun PostScreen(modifier: Modifier = Modifier, viewModel: PostViewModel = PostViewModel()) {
    val context = LocalContext.current
    val lifecycleOwner = LocalLifecycleOwner.current

    // æ£€æŸ¥ç›¸æœºæƒé™
    var hasCameraPermission by remember {
        mutableStateOf(
            ContextCompat.checkSelfPermission(
                context,
                Manifest.permission.CAMERA
            ) == PackageManager.PERMISSION_GRANTED
        )
    }

    // æ£€æŸ¥éº¦å…‹é£æƒé™
    var hasMicrophonePermission by remember {
        mutableStateOf(
            ContextCompat.checkSelfPermission(
                context,
                RECORD_AUDIO
            ) == PackageManager.PERMISSION_GRANTED
        )
    }

    // åŒæ—¶è¯·æ±‚ç›¸æœºå’Œéº¦å…‹é£æƒé™
    val multiplePermissionLauncher = rememberLauncherForActivityResult(
        contract = ActivityResultContracts.RequestMultiplePermissions()
    ) { permissions ->
        hasCameraPermission = permissions[Manifest.permission.CAMERA] == true
        hasMicrophonePermission = permissions[RECORD_AUDIO] == true
    }

    // è‡ªåŠ¨ç”³è¯·æƒé™
    LaunchedEffect(Unit) {
        if (!hasCameraPermission || !hasMicrophonePermission) {
            multiplePermissionLauncher.launch(arrayOf(Manifest.permission.CAMERA, RECORD_AUDIO))
        }
    }

    // éŸ³é¢‘å½•åˆ¶çŠ¶æ€
    var isRecording by remember { mutableStateOf(false) }
    var isPlaying by remember { mutableStateOf(false) }
    // è¯­éŸ³è¯†åˆ«ç›¸å…³
    var isRecognizing by remember { mutableStateOf(false) }
    var recognizedText by remember { mutableStateOf("") }
    var speechRecognizer: SpeechRecognizer? = remember { null }
    // æ–‡å­—è½¬è¯­éŸ³ç›¸å…³
    var textToSpeech: TextToSpeech? = remember { null }
    // éœ‡åŠ¨ç›¸å…³
    var vibrator: Vibrator? = remember { null }
    // åŠ è½½çŠ¶æ€
    var isLoading by remember { mutableStateOf(false) }
    // é”™è¯¯ä¿¡æ¯
    var errorMessage by remember { mutableStateOf<String?>(null) }
    // åç¨‹ä½œç”¨åŸŸ
    val coroutineScope = rememberCoroutineScope()
    // å†å²voice_text
    var historyVoiceText by remember { mutableStateOf("") }
    // è‡ªåŠ¨å‘é€å¤„ç†å™¨
    var autoSendHandler: Handler? = remember { null }
    // ä»»åŠ¡å®ŒæˆçŠ¶æ€
    var isTaskComplete by remember { mutableStateOf(false) }
    // ç‚¹å‡»ä½ç½®å’Œå…‰æ™•çŠ¶æ€
    var isPressed by remember { mutableStateOf(false) }
    var pressPosition by remember { mutableStateOf(Offset.Zero) }
    // è§£æåçš„voice_text
    var voiceText by remember { mutableStateOf("") }

    // å­˜å‚¨å½•åˆ¶çš„éŸ³é¢‘æ•°æ®
    val recordedAudioData = remember { mutableListOf<ByteArray>() }
    // ç›¸æœºé¢„è§ˆè§†å›¾
    val previewView = remember { PreviewView(context) }
    var imageCapture: ImageCapture? = remember { null }
    var cameraProvider: ProcessCameraProvider? = remember { null }

    // åˆå§‹åŒ–ç›¸æœº
    fun initializeCamera() {
        val cameraProviderFuture = ProcessCameraProvider.getInstance(context)
        cameraProviderFuture.addListener({ 
            val provider = cameraProviderFuture.get()
            cameraProvider = provider
            
            val preview = Preview.Builder().build().also {
                it.setSurfaceProvider(previewView.surfaceProvider)
            }

            imageCapture = ImageCapture.Builder().build()

            val cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA

            try {
                provider.unbindAll()
                provider.bindToLifecycle(
                    lifecycleOwner as LifecycleOwner,
                    cameraSelector,
                    preview,
                    imageCapture
                )
                Log.d("CameraX", "ç›¸æœºåˆå§‹åŒ–æˆåŠŸ")
            } catch (exc: Exception) {
                Log.e("CameraX", "Use case binding failed", exc)
                errorMessage = "ç›¸æœºåˆå§‹åŒ–å¤±è´¥: ${exc.message}"
            }
        }, ContextCompat.getMainExecutor(context))
    }

    // åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
    fun initializeSpeechRecognizer() {
        if (SpeechRecognizer.isRecognitionAvailable(context)) {
            speechRecognizer = SpeechRecognizer.createSpeechRecognizer(context)
            speechRecognizer?.setRecognitionListener(object : RecognitionListener {
                override fun onReadyForSpeech(params: Bundle?) {
                    Log.d("SpeechRecognition", "å‡†å¤‡å°±ç»ª")
                }

                override fun onBeginningOfSpeech() {
                    Log.d("SpeechRecognition", "å¼€å§‹è¯´è¯")
                }

                override fun onRmsChanged(rmsdB: Float) {
                }

                override fun onBufferReceived(buffer: ByteArray?) {
                }

                override fun onEndOfSpeech() {
                    Log.d("SpeechRecognition", "ç»“æŸè¯´è¯")
                }

                override fun onError(error: Int) {
                    Log.e("SpeechRecognition", "é”™è¯¯: $error")
                    isRecognizing = false
                }

                override fun onResults(results: Bundle?) {
                    val matches = results?.getStringArrayList(SpeechRecognizer.RESULTS_RECOGNITION)
                    if (!matches.isNullOrEmpty()) {
                        recognizedText = matches[0]
                        Log.d("SpeechRecognition", "è¯†åˆ«ç»“æœ: ${matches[0]}")
                    }
                    isRecognizing = false
                }

                override fun onPartialResults(partialResults: Bundle?) {
                    val matches = partialResults?.getStringArrayList(SpeechRecognizer.RESULTS_RECOGNITION)
                    if (!matches.isNullOrEmpty()) {
                        recognizedText = matches[0]
                        Log.d("SpeechRecognition", "éƒ¨åˆ†ç»“æœ: ${matches[0]}")
                    }
                }

                override fun onEvent(eventType: Int, params: Bundle?) {
                }
            })
        } else {
            errorMessage = "è®¾å¤‡ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«"
        }
    }

    // åˆå§‹åŒ–æ–‡å­—è½¬è¯­éŸ³
    fun initializeTextToSpeech() {
        textToSpeech = TextToSpeech(context) {
            if (it == TextToSpeech.SUCCESS) {
                textToSpeech?.setLanguage(Locale.CHINA)
                Log.d("TextToSpeech", "åˆå§‹åŒ–æˆåŠŸ")
            } else {
                Log.e("TextToSpeech", "åˆå§‹åŒ–å¤±è´¥")
            }
        }
    }

    // åˆå§‹åŒ–éœ‡åŠ¨å™¨
    fun initializeVibrator() {
        vibrator = if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.S) {
            context.getSystemService(Vibrator::class.java)
        } else {
            @Suppress("DEPRECATION")
            context.getSystemService(Context.VIBRATOR_SERVICE) as Vibrator
        }
    }

    // æ–‡å­—è½¬è¯­éŸ³
    fun speakText(text: String) {
        if (textToSpeech == null) {
            initializeTextToSpeech()
        }
        textToSpeech?.speak(text, TextToSpeech.QUEUE_FLUSH, null, null)
    }

    // æ ¹æ®vibration_modeè¿›è¡Œéœ‡åŠ¨
    fun vibrateBasedOnMode(mode: String) {
        if (vibrator == null) {
            initializeVibrator()
        }

        val pattern = when (mode) {
            "low_freq" -> longArrayOf(0, 200, 100, 200) // ä½é¢‘éœ‡åŠ¨
            "high_freq" -> longArrayOf(0, 50, 50, 50, 50, 50) // é«˜é¢‘éœ‡åŠ¨
            "swipe_left" -> longArrayOf(0, 100, 50, 100, 50, 200) // å‘å·¦æ»‘åŠ¨éœ‡åŠ¨
            "swipe_right" -> longArrayOf(0, 200, 50, 100, 50, 100) // å‘å³æ»‘åŠ¨éœ‡åŠ¨
            else -> longArrayOf(0, 100) // é»˜è®¤éœ‡åŠ¨
        }

        if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.O) {
            val vibrationEffect = android.os.VibrationEffect.createWaveform(pattern, -1)
            vibrator?.vibrate(vibrationEffect)
        } else {
            @Suppress("DEPRECATION")
            vibrator?.vibrate(pattern, -1)
        }
    }

    
    // æ‹ç…§å¹¶å‘é€ç»™AI
    fun takePhotoAndSendToAI(includeHistory: Boolean = false) {
        isLoading = true
        errorMessage = null

        // é‡æ–°åˆå§‹åŒ–ç›¸æœºä»¥ç¡®ä¿å¯ç”¨
        initializeCamera()

        // ç­‰å¾…ç›¸æœºåˆå§‹åŒ–å®Œæˆ
        Handler(Looper.getMainLooper()).postDelayed({
            val imageCapture = imageCapture ?: run {
                errorMessage = "ç›¸æœºæœªåˆå§‹åŒ–"
                isLoading = false
                return@postDelayed
            }

            // åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            val photoFile = File.createTempFile("temp_photo", ".jpg", context.cacheDir)
            val outputFileOptions = ImageCapture.OutputFileOptions.Builder(photoFile).build()

            // æ‹ç…§
            imageCapture.takePicture(
                outputFileOptions,
                ContextCompat.getMainExecutor(context),
                object : ImageCapture.OnImageSavedCallback {
                    override fun onImageSaved(outputFileResults: ImageCapture.OutputFileResults) {
                        try {
                            // å°†å›¾ç‰‡è½¬æ¢ä¸ºBase64
                            val base64Image = bitmapToBase64(photoFile)
                            if (base64Image.isNullOrEmpty()) {
                                errorMessage = "å›¾ç‰‡è½¬æ¢å¤±è´¥"
                                isLoading = false
                                // åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                                photoFile.delete()
                                return
                            }

                            // æ„å»ºè¯·æ±‚æ¶ˆæ¯
                            val userMessages = mutableListOf<Content>()
                            
                            // æ·»åŠ å›¾ç‰‡
                            userMessages.add(
                                Content(
                                    type = "image_url",
                                    image_url = ImageUrl(url = "data:image/jpeg;base64,$base64Image")
                                )
                            )
                            
                            // æ·»åŠ æç¤ºè¯
                            userMessages.add(
                                Content(
                                    type = "text",
                                    text = context.getString(R.string.ai_prompt_what)
                                )
                            )
                            userMessages.add(
                                Content(
                                    type = "text",
                                    text = "è¯·å¸¦æˆ‘å»ç¶å°å‰."
                                )
                            )
                            // å¦‚æœéœ€è¦åŒ…å«å†å²voice_text
                            if (includeHistory && historyVoiceText.isNotEmpty()) {
                                userMessages.add(
                                    Content(
                                        type = "text",
                                        text = "å†å²è¯­éŸ³æŒ‡ä»¤ï¼Œä»…ä½œä¸ºå‚è€ƒ: $historyVoiceText"
                                    )
                                )
                            }

                            // å‘é€ç»™AI
                            viewModel.fetchPost("qwen3-vl-flash", userMessages, "sk-ee10fa059ce846468490b65eb61a278a")

                            // åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                            photoFile.delete()
                        } catch (e: Exception) {
                            Log.e("PhotoAI", "å¤„ç†å›¾ç‰‡å¤±è´¥", e)
                            errorMessage = "å¤„ç†å›¾ç‰‡å¤±è´¥: ${e.message}"
                            // åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                            photoFile.delete()
                        } finally {
                            isLoading = false
                        }
                    }

                    override fun onError(exception: ImageCaptureException) {
                        Log.e("PhotoAI", "æ‹ç…§å¤±è´¥", exception)
                        errorMessage = "æ‹ç…§å¤±è´¥: ${exception.message}"
                        isLoading = false
                    }
                }
            )
        }, 500)
    }
    // å¤„ç†AIå“åº”çš„JSON
    fun processAIResponseJson(jsonString: String) {
        try {
            // å»é™¤å¯èƒ½çš„```json```å’Œ```æ ‡è®°
            val cleanedJsonString = jsonString
                .trim()
                .removePrefix("```json")
                .removeSuffix("```")
                .trim()
            
            val gson = Gson()
            val aiResponse = gson.fromJson(cleanedJsonString, AIResponseJson::class.java)

            // ä¿å­˜voice_textåˆ°çŠ¶æ€å˜é‡
            voiceText = aiResponse.voice_text

            // 1. å°†voice_textè½¬è¯­éŸ³
            speakText(aiResponse.voice_text)

            
            // 2. ä¿å­˜å†å²voice_text
            historyVoiceText +=aiResponse.voice_text+","
            // 3. æ ¹æ®vibration_modeè¿›è¡Œéœ‡åŠ¨
            // 4. å¦‚æœis_task_completeä¸ºå‡ï¼Œå®šæ—¶è‡ªåŠ¨å‘é€å›¾ç‰‡
            if (!aiResponse.is_task_complete) {
                vibrateBasedOnMode(aiResponse.vibration_mode)
                autoSendHandler?.removeCallbacksAndMessages(null)
                autoSendHandler = Handler(Looper.getMainLooper())
                autoSendHandler?.postDelayed({
                    takePhotoAndSendToAI(true)
                }, aiResponse.next_transmission_ms.toLong())
            } else {
                isTaskComplete = true
                autoSendHandler?.removeCallbacksAndMessages(null)
            }
        } catch (e: Exception) {
            Log.e("AIResponse", "è§£æJSONå¤±è´¥", e)
            errorMessage = "è§£æAIå“åº”å¤±è´¥: ${e.message}"
        }
    }

    // å¼€å§‹è¯­éŸ³è¯†åˆ«
    fun startSpeechRecognition() {
        if (speechRecognizer == null) {
            initializeSpeechRecognizer()
        }

        val intent = Intent(RecognizerIntent.ACTION_RECOGNIZE_SPEECH)
        intent.putExtra(RecognizerIntent.EXTRA_LANGUAGE_MODEL, RecognizerIntent.LANGUAGE_MODEL_FREE_FORM)
        intent.putExtra(RecognizerIntent.EXTRA_LANGUAGE, Locale.CHINA.toString())
        intent.putExtra(RecognizerIntent.EXTRA_PARTIAL_RESULTS, true)

        try {
            speechRecognizer?.startListening(intent)
            isRecognizing = true
            // æ¸…ç©ºä¹‹å‰çš„æ–‡å­—
            recognizedText = ""
        } catch (e: Exception) {
            Log.e("SpeechRecognition", "å¯åŠ¨å¤±è´¥", e)
            errorMessage = "è¯­éŸ³è¯†åˆ«å¯åŠ¨å¤±è´¥: ${e.message}"
            isRecognizing = false
        }
    }

    // åœæ­¢è¯­éŸ³è¯†åˆ«
    fun stopSpeechRecognition() {
        speechRecognizer?.stopListening()
        isRecognizing = false
    }

    // é¦–æ¬¡åˆå§‹åŒ–ç›¸æœº
    LaunchedEffect(Unit) {
        initializeCamera()
    }


    Box(modifier = modifier.fillMaxSize()) {
        Column {
            // æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
            if (!errorMessage.isNullOrEmpty()) {
                Text(errorMessage!!, color = MaterialTheme.colorScheme.error)
            }

            // è¯­éŸ³è¯†åˆ«çŠ¶æ€æç¤º
            if (isRecognizing) {
                Text(
                    text = "ğŸ¤ æ­£åœ¨è†å¬ï¼ˆç‚¹å‡»/é•¿æŒ‰è¯´è¯ï¼‰",
                    modifier = Modifier
                        .fillMaxWidth()
                        .padding(8.dp),
                    color = MaterialTheme.colorScheme.primary,
                    textAlign = TextAlign.Center
                )
            }

            // æ˜¾ç¤ºAIå“åº”
            val currentResult = viewModel._postState.value?.choices?.firstOrNull()?.message?.content
            Log.d("currentResult", "currentResult: $currentResult")

            // ç›‘å¬AIå“åº”å˜åŒ–å¹¶å¤„ç†
            LaunchedEffect(currentResult) {
                if (!currentResult.isNullOrEmpty()) {
                    // å¤„ç†AIå“åº”çš„JSON
                    processAIResponseJson(currentResult)
                }
            }

            if (voiceText.isNotEmpty()) {
                Text(voiceText)
            } else if (!isLoading && errorMessage.isNullOrEmpty()) {
                Text("å¥½çš„ï¼Œä¸ºä½ å¯¼èˆªåˆ°å¨æˆ¿ç¶å°ã€‚")
            }

            // æ˜¾ç¤ºç›¸æœºé¢„è§ˆ
            Box(
                modifier = Modifier
                    .weight(1f)
            ) {
                AndroidView(
                    factory = { previewView },
                    modifier = Modifier
                        .fillMaxSize()
                        .pointerInput(Unit) {
                            detectTapGestures(
                                /*onTap = { offset ->
                                    if (hasMicrophonePermission) {
                                        pressPosition = offset
                                        isPressed = true
                                        recognizedText = "æ­£åœ¨è¯†åˆ«..."
                                        startSpeechRecognition()
                                        coroutineScope.launch {
                                            delay(3000)
                                            stopSpeechRecognition()
                                            isPressed = false
                                        }
                                    } else {
                                        errorMessage = "è¯·å…ˆæˆäºˆéº¦å…‹é£æƒé™"
                                    }
                                },*/
                                onPress = { offset ->
                                    if (hasMicrophonePermission) {
                                        pressPosition = offset
                                        isPressed = true
                                        // æŒ‰ä¸‹æ—¶éœ‡åŠ¨
                                        vibrateBasedOnMode("low_freq")
                                        recognizedText = "é•¿æŒ‰è¯†åˆ«ä¸­..."
                                        startSpeechRecognition()
                                        tryAwaitRelease()
                                        stopSpeechRecognition()
                                        // æŠ¬èµ·æ—¶éœ‡åŠ¨
                                        vibrateBasedOnMode("low_freq")
                                        isPressed = false
                                        takePhotoAndSendToAI()
                                    } else {
                                        errorMessage = "è¯·å…ˆæˆäºˆéº¦å…‹é£æƒé™"
                                    }
                                }
                            )
                        }
                )

                // å…‰æ™•æ•ˆæœ
                if (isPressed) {
                    androidx.compose.foundation.Canvas(
                        modifier = Modifier.fillMaxSize()
                    ) {
                        drawCircle(
                            color = Color.Blue.copy(alpha = 0.3f),
                            radius = 50f,
                            center = pressPosition
                        )
                    }
                }
            }

            // å½•éŸ³æ§åˆ¶æŒ‰é’®å·²éšè—
            /*
            if (hasMicrophonePermission) {
                Column(
                    modifier = Modifier.fillMaxWidth(),
                    horizontalAlignment = Alignment.CenterHorizontally
                ) {
                    // é•¿æŒ‰å½•éŸ³æŒ‰é’®
                    Box(
                        modifier = Modifier
                            .pointerInput(Unit) {
                                detectTapGestures(
                                    onPress = {
                                        // é•¿æŒ‰å¼€å§‹å½•éŸ³å’Œè¯­éŸ³è¯†åˆ«
                                        startSpeechRecognition()
                                        // ç­‰å¾…ç”¨æˆ·é‡Šæ”¾
                                        tryAwaitRelease()
                                        // é‡Šæ”¾ååœæ­¢è¯†åˆ«
                                        stopSpeechRecognition()
                                    },
                                    onTap = {
                                        // ç‚¹å‡»äº‹ä»¶ï¼šåˆ·æ–°æ–‡å­—
                                        recognizedText = ""
                                    }
                                )
                            }
                    ) {
                        Button(
                            modifier = Modifier
                                .width(200.dp)
                                .height(60.dp),
                            onClick = {
                                // ç‚¹å‡»äº‹ä»¶ï¼šåˆ·æ–°æ–‡å­—
                                recognizedText = ""
                            }
                        ) {
                            Text(
                                if (isRecognizing) "æ­£åœ¨è¯†åˆ«..." else "é•¿æŒ‰å½•éŸ³",
                                textAlign = TextAlign.Center
                            )
                        }
                    }

                    // æ˜¾ç¤ºè¯†åˆ«çš„æ–‡å­—
                    if (recognizedText.isNotEmpty()) {
                        Card(
                            modifier = Modifier
                                .fillMaxWidth()
                                .padding(16.dp)
                                .wrapContentHeight(),
                            elevation = CardDefaults.cardElevation(defaultElevation = 4.dp)
                        ) {
                            Text(
                                text = recognizedText,
                                modifier = Modifier.padding(16.dp),
                                style = MaterialTheme.typography.bodyLarge
                            )
                        }
                    }

                    // æ’­æ”¾å½•éŸ³æŒ‰é’®
                    Button(
                        onClick = {
                            if (!isPlaying && recordedAudioData.isNotEmpty()) {
                                playRecordedAudio(recordedAudioData) {
                                    isPlaying = false // æ’­æ”¾å®Œæˆåé‡ç½®çŠ¶æ€
                                }
                                isPlaying = true
                            }
                        },
                        modifier = Modifier
                            .width(200.dp)
                            .height(60.dp)
                            .padding(top = 8.dp)
                    ) {
                        Text("æ’­æ”¾å½•éŸ³")
                    }
                }
            }
            */


        }
    }
}

// å°†å›¾ç‰‡æ–‡ä»¶è½¬æ¢ä¸ºBase64ç¼–ç 
private fun bitmapToBase64(file: File): String? {
    return try {
        val bitmap = BitmapFactory.decodeFile(file.absolutePath)
        val byteArrayOutputStream = ByteArrayOutputStream()
        bitmap.compress(Bitmap.CompressFormat.JPEG, 80, byteArrayOutputStream)
        val byteArray = byteArrayOutputStream.toByteArray()
        Base64.encodeToString(byteArray, Base64.NO_WRAP)
    } catch (e: Exception) {
        Log.e("Base64", "è½¬æ¢å¤±è´¥", e)
        null
    }
}




private var audioRecord: AudioRecord? = null
private var recordingThread: Thread? = null
private var isRecordingActive = false

// å°† Context ä½œä¸ºå‚æ•°ä¼ å…¥ï¼Œé¿å…åœ¨é Composable å‡½æ•°ä¸­ä½¿ç”¨ LocalContext.current
private fun startRecording(audioBuffer: MutableList<ByteArray>, context: android.content.Context) {
    // æ£€æŸ¥éº¦å…‹é£æƒé™
    if (ContextCompat.checkSelfPermission(
            context,
            RECORD_AUDIO
        ) != PackageManager.PERMISSION_GRANTED) {
        Log.w("AudioRecord", "No microphone permission granted")
        return
    }

    // æ¸…ç©ºä¹‹å‰çš„å½•éŸ³æ•°æ®
    audioBuffer.clear()

    val audioSource = MediaRecorder.AudioSource.MIC
    val sampleRate = 44100
    val channelConfig = AudioFormat.CHANNEL_IN_MONO
    val audioFormat = AudioFormat.ENCODING_PCM_16BIT

    val bufferSize = AudioRecord.getMinBufferSize(sampleRate, channelConfig, audioFormat)

    audioRecord = AudioRecord(
        audioSource,
        sampleRate,
        channelConfig,
        audioFormat,
        bufferSize
    ).apply {
        startRecording()
        isRecordingActive = true

        recordingThread = Thread {
            val buffer = ByteArray(bufferSize)
            while (isRecordingActive) {
                val bytesRead = read(buffer, 0, bufferSize)
                if (bytesRead > 0) {
                    val dataCopy = buffer.copyOf(bytesRead)
                    audioBuffer.add(dataCopy)
                }
            }
        }
        recordingThread?.start()
    }
}

private fun playRecordedAudio(
    audioData: List<ByteArray>,
    onPlaybackFinished: () -> Unit // æ·»åŠ å›è°ƒå‚æ•°
) {
    // æ£€æŸ¥æ˜¯å¦æœ‰éŸ³é¢‘æ•°æ®
    if (audioData.isEmpty()) {
        Log.w("AudioPlay", "No audio data to play")
        return
    }

    val audioTrackThread = Thread {
        val sampleRate = 44100
        val channelConfig = AudioFormat.CHANNEL_OUT_MONO
        val audioFormat = AudioFormat.ENCODING_PCM_16BIT

        // è®¡ç®—æ€»å¤§å°
        val totalSize = audioData.sumOf { it.size }
        val playbackBufferSize = AudioTrack.getMinBufferSize(sampleRate, channelConfig, audioFormat)

        // ç¡®ä¿ç¼“å†²åŒºå¤§å°è¶³å¤Ÿå¤§
        val finalBufferSize = Math.max(playbackBufferSize, totalSize)

        val audioTrack = AudioTrack(
            AudioManager.STREAM_MUSIC,
            sampleRate,
            channelConfig,
            audioFormat,
            finalBufferSize,
            AudioTrack.MODE_STATIC
        )

        // å°†å½•åˆ¶çš„æ•°æ®å¤åˆ¶åˆ° AudioTrack
        val combinedData = ByteArray(totalSize)
        var offset = 0
        for (chunk in audioData) {
            System.arraycopy(chunk, 0, combinedData, offset, chunk.size)
            offset += chunk.size
        }

        val writeResult = audioTrack.write(combinedData, 0, combinedData.size)
        if (writeResult > 0) {
            audioTrack.play()

            // ç­‰å¾…æ’­æ”¾å®Œæˆ
            while (audioTrack.playState == AudioTrack.PLAYSTATE_PLAYING) {
                Thread.sleep(100)
            }
        } else {
            Log.e("AudioPlay", "Failed to write audio data to track")
        }

        audioTrack.release()

        // è°ƒç”¨å›è°ƒæ›´æ–°çŠ¶æ€
        Handler(Looper.getMainLooper()).post {
            onPlaybackFinished()
        }
    }

    audioTrackThread.start()
}




private fun stopRecording() {
    isRecordingActive = false
    audioRecord?.apply {
        stop()
        release()
    }
    audioRecord = null
    recordingThread?.interrupt()
    recordingThread = null
}

//val lifecycleOwner = androidx.lifecycle.compose.LocalLifecycleOwner.current